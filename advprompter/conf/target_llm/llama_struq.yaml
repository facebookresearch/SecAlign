defaults:
  - base_target_llm
  - _self_

llm_params:
  model_name: "llama-struq"
  checkpoint: '../models/llama-7b_SpclSpclSpcl_NaiveCompletion_2024-02-02-00-00-00' # or replace with local DIR
prompt_manager:
  prompt_template:
    - key: system_message
      msg: "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n"
    - key: full_instruct
      msg: "{full_instruct}"  # loaded from context
    - key: separator
      msg: "\n\n[MARK] [RESP][COLN]\n"
    - key: target
      msg: "{target}"  # loaded from context